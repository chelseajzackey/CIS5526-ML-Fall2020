{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chelsea Zackey\n",
    "\n",
    "CIS 5526: HW 2\n",
    "\n",
    "9/24/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Supervised Learning - k-Nearest Neighbor (kNN) Algorithm \n",
    "\n",
    "k-Nearest Neighbor (kNN) algorithm uses a simple idea: \"you are what your neighbors are\". In the first part of the assignment, we will cover some background needed to understand the kNN algorithm. In the second part, you will be asked to apply your knowledge on another data set. \n",
    "\n",
    "## Part A: kNN Tutorial with Questions (50% of grade)\n",
    "\n",
    "Let us start by importing the needed libraries. We will continue using the sklearn library, which implements many of the most popular data science algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the Iris data set using a sklearn function `load_iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html, `iris` is an object with features `data` (a 150x4 matrix, where $i$-th row are 4 features of the $i$-th flower), `feature_names` (the names of the 4 features), `target` (a vector of length 150, where $i$-th number is the type of the $i$-th flower -- in machine learning people often say \"label\" instead of \"target\"), `target_names` (these are strings explaining what each of the 3 types of flowers are), and `DESCR` (giving some information about the Iris data set). Let us list them all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)\n",
    "print(iris.data)\n",
    "print(iris.feature_names)\n",
    "print(iris.target)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the features of the second flower are `[4.9, 3.0, 1.4, 0.2]`, which means its `sepal_length` is 4.9 cm, `sepal_width` is 3.0 cm, `petal_length` is 1.4 cm, and `petal_width` is 0.2 cm. We will write it matematically as $x_2 = [x_{21}, x_{22}, x_{23}, x_{24}] = [4.9, 3.0, 1.4, 0.2]$. We see that its `target` is 0, which means the type of this iris is `setosa`. We will write it matematically as $y_2 = 0$. All this information was obtained by real botanists who studied iris flowers trying to understand the physical measurements that discriminate between the 3 different types of those flowers.\n",
    "\n",
    "In machine learning, people like to denote this data set as $D_{Iris} = \\{(x_i, y_i), i = 1, 2 ... 150\\}$, meaning that data set $D_{Iris}$ is a set of 150 labeled examples $(x_i, y_i)$. An alternative is to write $D_{Iris} = \\{X_{Iris}, Y_{Iris}\\}$.\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning is a game with the following objective. You are given the iris data set $D_{Iris}$ where you know 4 features and target values for 150 irises and your objective is to come up with a computer program that predicts a type of any iris flower given the values of its 4 attributes. Written in pseudocode, this is what you have to do:\n",
    "\n",
    "`predictor = create(algorithm_type, D)\n",
    "y_new = predictor(x_new)`\n",
    "\n",
    "In the first line, you are running a `create` function that takes as input data set `D` and the name of a supervised learning algorithm `algorithm_type` and produces as an output a computer program `predictor`. In the second line, you are using `predictor` to predict the label (`y_new` value) for a flower whose features are given by `x_new`.\n",
    "\n",
    "### kNN Algorithm\n",
    "kNN is a popular supervised learning algorithm that allows us to create `predictor`. The idea of kNN is that the label of flower `x_new` depends on labels of flowers in its neighborhood. In particular, kNN finds the distance between `x_new` and every example `x` in data set `D`. Then, it looks at the label `y` of k examples which are the closest to `x_new`. The predicted label `y_new` is obtained as the most common label in the group of the k nearest neighbors.\n",
    "\n",
    "**Parameter choice**. We need to make a few decisions when running kNN. The most important is the choice of `k`. If `k = 1`, then we are looking only at the hearest neighbor and it might not be a good idea if we are dealing with noisy data. If `k` is very large, then we might be counting far neighbors that might have different properties. Other decisions include the choice of distance metric (Euclidean is the standard one) and the choice whether to weight closer neighbors more than the farther ones.\n",
    "\n",
    "**Accuracy**. When deciding which parameters to pick or which supervised learning algorithm to use (there are popular algorithms other than kNN), the question is how to measure which choice is better. The answer is to check if `predictor` provides accurate prediction. Given a data set `D`, a typical way to check accuracy is to randomly split `D` into two data sets, `D_train` and `D_test`. Then, `predictor` is created/trained using `D_train` data set and its accuracy is checked using `D_test`. In particular, we use `predictor` to predict label of every example from `D_test` and compare it with the true labels. The percentage of the correct guesses on `D_test` is reported as accuracy of `predictor`.\n",
    "\n",
    "## kNN Demo\n",
    "The following piece of code is taken from:\n",
    "http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py. Let us run it (it is guaranteed to run with Python 2.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#k = 1   # how many nearest neighbors are consulted\n",
    "#k = 3 # change total nearest neighbors (Question 1)\n",
    "k = 25 # change total nearest neighbors (Question 2)\n",
    "\n",
    "X = iris.data[:, [0,1]]  # we only take the first two features. We could\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "#clf = neighbors.KNeighborsClassifier(k, weights='distance') # Question 3 -CZ\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(6,3))   # this makes both axis equal \n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (k, 'uniform'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting figure shows the predictions of kNN when $k=1$. If `x_new` is in the blue region, the prediction will be the blue class. From this picture, we can observe a small blue blobs inside the predominantly gray area. This is because the nearest neighbor in this area is the blue example. \n",
    "\n",
    "**Question 1**. Change value of k to 3 and observe if there is any difference. Discuss what you see and why.\n",
    "\n",
    "**Soln 1**: After initially having k set to 1, we see that upon setting k=3, the boundaries of the areas predicted to belong to each target label become more refined; i.e., smoother with less irregularities (curves, \"islands\"/\"peninsulas\", \"spotting\", etc.). Focusing solely on the scatter plot, we see that our known Iris-Versicolour (target=1/\"green\") target points and Iris-Virginica (target=2/\"blue\") target points share significant similarities in their reported values for the two features we examine (sepal length and width); in other words, it appears these two classes of irises cannot be readily distinguished from their sepals alone, as they are oftentimes too similar. Moreover, this fact leads to the complex enmeshment of the decision boundaries drawn for these classes above due to perceived \"outliers\" from each class. Hence, upon increasing the neighborhood size from 1 to 3, we notice a change particularly around these outlier points, as their influence on the predicted labels for the areas of the graph surrounding them become overruled by the predominant class in that area. \n",
    "\n",
    "**Question 2**. Change k to an even higher value, let us say to 25. What do we see now? Discuss.\n",
    "\n",
    "**Soln 2**: Upon changing k to 25, we see even less irregularities (and more smoothness) along the decision boundaries drawn up between the green and blue target classes. However, this comes at the cost of decreased prediction accuracy when determining the labels of data points belonging to both classes that fall within the areas of increased enmeshment. Intuitively, when increasing the neighborhood drawn up for these points past a certain size, we can expect the influence of the neighbors sharing the same label of a given point to be overshadowed by that of neighbors with different labels, depending on where exactly our target point falls on the graph, thus leading to this decrease in accuracy.  \n",
    "\n",
    "**Queston 3**. In the line that creates `clf` change weights='uniform' to weights='distance'. Check the documentation or google to understand what it means. Explain. Run the code and discuss if you see any difference.\n",
    "\n",
    "**Soln. 3**: Upon switching `weights` from the `uniform` setting to the `distance` setting, our classifier will place greater importance on the targets of the neighbors closest to our examined data point out of all k neighbors we consider. This is accomplished by attaching a weight to a neighbor's influence based on the inverse of its distance from our target point. Thus, instead of making predictions based on the majority target class among the k neighbors examined for a given point (as when `weights='uniform'`), the classifier chooses the target label of greatest weight within the given neighborhood. \n",
    "\n",
    "When `k=1`, there appears to be no difference (as expected, for weights don't matter when we only examine 1 neighbor). Upon increasing `k=3`, however, we do see that adjusting prediction weights in this way does have an affect on the decision boundaries plotted by our classifier: for example, we can see adjustments in the decision boundaries drawn where \"outliers\" of a target class are found in an area of the graph where members of a different class are more predominant. Since the outlier would be considered one of the closest neighbors of any point falling in the surrounding area, the classifier may adjust its predicted labels for the subset of points for which this outlier \"outweighs\" the influence of all other k-1 nearest neighbors. Hence, the outcome here is a greater radius around certain outliers sharing the same target label, instead of that of the neighborhood majority. \n",
    "\n",
    "**Question 4**. Take a look at the code and try to understand what each line of the code does. Explain each line of code.\n",
    "\n",
    "**Soln 4**: \n",
    "`k = 1\n",
    "X = iris.data[:, [0,1]]\n",
    "y = iris.target\n",
    "h = .02` \n",
    "\n",
    "In the above lines of code, we specify important parameters used by our kNN claassifier, particularly the number of neighbors examined, the data set of examples we consult (in this case, we limit our analysis to the first two features listed in `iris.feature_names`: sepal length and sepal width), the targets corresponding to each example, and the interval length between the tick marks included in the axis used for the plot we later generate. \n",
    "\n",
    "`cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])`\n",
    "\n",
    "The next lines of code above are used to indicate the colors we choose to mark the data points belonging to each target class (`cmap_bold`), and the areas of the graph where each target class lives (`cmap_light`), according to the predictions of our kNN classifier. Each color is assigned to the target class that shares the same index in `iris.target_names` (e.g., setosa irises and the area they comprise on the graph will be displayed in red). \n",
    "\n",
    "`clf = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "clf.fit(X, y)`\n",
    "\n",
    "The next two lines of code above first instantiate the `KNeighborsClassifier` class using our identifier `clf`, then use the iris data set `X` with targets `y` to \"fit\" the model, i.e., use these known values in `X` and `y` as training data to base the classifier's predictions off of. \n",
    "\n",
    "`x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])`\n",
    "\n",
    "The first two lines above now identify the minimum and maximum values reported for the first feature (sepal length) and second feature (sepal width) that we examine in our data set, which are used in tandem with our step size `h` to create our graph's x and y axes, respectively, in the following line of code. The last line above uses the variable `Z` to store the predicted labels comprising each area of the shape created by our x and y axes (`xx` and `yy`) as determined by our classifier `clf`. The parameter `np.c_[xx.ravel(), yy.ravel()]` is the shape we provide for `clf` by first smoothing out our axes (making them appear continous) and using them to conjure up a 2D plane. \n",
    "\n",
    "`Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(6,3)) \n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)`\n",
    "\n",
    "In the following lines of code above, we begin to generate a color map representation of our predictions: first, we recalibrate our predictions `Z` to share the same shape as our x axis `xx`. Next, we instantiate a new plot figure and adjust its figure size. Finally, we create a 2D color map of the predictions in `Z` (with values for sepal length and width measured on the x and y axes, respectively) using the target class colors specified in `cmap_light`. \n",
    "\n",
    "`plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (k, 'uniform'))\n",
    "plt.show()`\n",
    "\n",
    "Finally, the last lines of codes first generate a scatter plot to overlay the color map in our plot figure, using sepal length and width as the x and y coordinates of each data point, respectively. Furthermore, the iris target values stored in `y` are used to determine the color of each point, and the choices of which are specified in `cmap_bold`. Next, the code sets the limits of the axes using our specified min and max values reported by each of the two features, attaches a title to our plot, then prints it for us to view. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "  As mentioned above, the typical mechanism for testing accuracy of a `predictor` is to split the data randomly into training and testing, train `predictor` on training data and test its performance on test data. Let us see how it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "# print(len(y_test)) #verification for Question 5 -CZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**. What is the size of the resulting objects?\n",
    "\n",
    "**Soln 5**: We are originally given a data set comprising 150 entries; thus, upon specifying test size of 0.33 (one-third of the size of our data set) for the method `train_test_split`, we should have 100 entries (a 100x4 matrix, accounting for our 4 features) for `X_train`, 100 targets for `y_train`, 50 entries (a 50x4 matrix) for `X_test`, and 50 targets for `y_test`. \n",
    "\n",
    "Now that we created training and test sets, we can train a kNN classifier using the training data. Before moving forward, let us take a second and take a look at the documentation for kNN implementation in sklearn: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Let us train the kNN predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#k = 1   # number of nearest neighbors\n",
    "#k = 3 # Question 9\n",
    "k = 5 #Question 9\n",
    "#k = 15 #Question 9\n",
    "#k = 25 # Question 9\n",
    "#k = 50 #Question 9\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "predictor.fit(X_train, y_train);\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained `predictor` we can use it to provide predictions on any example `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.  2.2]\n",
      " [5.2 4.1]\n",
      " [5.8 2.7]\n",
      " [5.4 3.7]]\n",
      "[(1, 2), (0, 0), (1, 1), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# select the first 4 test examples\n",
    "i = [0,1,2,3]\n",
    "x = X_test[i,:]\n",
    "print(x)\n",
    "# predict its label\n",
    "yhat = predictor.predict(x)\n",
    "# compare predicted and true labels\n",
    "result = zip(yhat, y_test[i])\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**. Did your kNN predictor do a good job in predicting labels of the first 4 test examples? \n",
    "\n",
    "**Soln 6**: Yes, the predictor determined the targets of 3 out of the 4 test examples correctly.\n",
    "\n",
    "**Question 7**. Write a piece of code that calculates the accuracy on those 4 test examples (number of correct guesses divided by the total number of guesses\n",
    "\n",
    "**Soln 7**: The below code has calculated 75% prediction accuracy for the first 4 test examples. \n",
    "\n",
    "**Question 8**. Find the predictions on all test examples in `X_test` and calculate the accuracy using your code from *Question 7*.\n",
    "\n",
    "**Soln 8**: Running the below code prints the predictions for all 50 test examples beside their corresponding true target values, along with the newly determined test accuracy of 80%.\n",
    "\n",
    "Pay attention that methods in sklearn.neighbors.KNeighborsClassifier allow you to test the accuracy in a faster way (you should not use it to answer Questions 7 and 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 accuracy: 0.75\n",
      "Test 2 results: \n",
      "[(1, 2), (0, 0), (1, 1), (0, 0), (0, 0), (0, 0), (2, 2), (0, 0), (2, 2), (1, 2), (1, 2), (0, 0), (2, 2), (2, 1), (0, 0), (2, 1), (0, 0), (1, 2), (0, 0), (1, 2), (0, 0), (2, 2), (1, 1), (1, 1), (1, 2), (1, 2), (1, 2), (2, 2), (1, 1), (1, 1), (2, 2), (0, 0), (1, 1), (0, 0), (2, 2), (2, 2), (2, 1), (2, 2), (2, 2), (0, 0), (2, 2), (2, 1), (1, 1), (0, 0), (2, 2), (2, 1), (0, 0), (1, 1), (2, 2), (0, 0)]\n",
      "Test 2 accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Code for question 7\n",
    "c = 0 # counter for correct predictions\n",
    "result = list(zip(yhat, y_test[i]))\n",
    "for j in range(len(result)):\n",
    "    if result[j][0] == result[j][1]:\n",
    "        c += 1\n",
    "print(\"Test 1 accuracy: \"+str(c/len(result)))\n",
    "\n",
    "#Code for question 8\n",
    "yhat = predictor.predict(X_test) # Find predictions for all test samples\n",
    "res = list(zip(yhat, y_test)) #join predictions with corresponding true values\n",
    "print(\"Test 2 results: \")\n",
    "print(res)\n",
    "c = 0 # counter for correct predictions\n",
    "for j in range(len(res)):\n",
    "    if res[j][0] == res[j][1]:\n",
    "        c += 1\n",
    "print(\"Test 2 accuracy: \"+str(c/len(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**. Train `predictor` using different choices of k. Try $k = 1, 3, 5, 15, 25, 50$. Report the accuracies on the test data (you can use the score method). Which choice of $k$ resulted in the highest accuracy? Comment briefly if the results make sense to you.\n",
    "\n",
    "**Soln 9**: The accuracies pertaining to each choice of `k` (all applied to the exact same testing & training instances of the data) are as follows:\n",
    "\n",
    "k=1: 0.72\n",
    "\n",
    "k=3: 0.78\n",
    "\n",
    "k=5: 0.75\n",
    "\n",
    "k=15: 0.74\n",
    "\n",
    "k=25: 0.74\n",
    "\n",
    "k=50: 0.68\n",
    "\n",
    "Based on the above statistics, it appears that when `k = 3`, our predictor achieves its greatest accuracy for the given test instance. This appears to make sense given our discussion above, regarding the fact that the green and blue iris classes share strong similarities in sepal shape & size (i.e., similar values in the reported sepal length & width features). In light of this, analyzing just one nearest neighborhood could lead to inaccuracies when predicting the targets of points that have strayed far from areas comprising a majority of members from their target class. By the same token, depending on just how far any given point or handful of points have strayed, it isn't helpful to examine a relatively large neighborhood either, as any neighbors of the same target class will begin to have less influence on the classifier's prediction. Based on what we see from the above scatter plot, most data points found along the boundary between the green and blue target areas are close enough to at least two other points of the same class to be labeled correctly upon setting `k=3`. \n",
    "\n",
    "**Question 10**. Other than choice of $k$, `KNeighborsClassifier` allows you to make some other choices. For example, in *Question 3* you saw that you can use a weighted prediction. There are few other options. Study the documentation and summarize in few sentences what other options you have when training the kNN classifier.\n",
    "\n",
    "**Soln 10**: Aside from those that we have already worked with, `KNeighborsClassifier` allows you adjust the metric you wish to use when defining \"nearness\" (along with necessary parameters), the algorithm you wish to use when finding nearest neighbors (along with necessary parameters), and the number of jobs you wish to create upon deciding to run neighbor searches in parallel. \n",
    "\n",
    "**Question 11**. Train kNN classifier on a different pair of features of your choice. Use $k$ of your choice and feel free to keep other choices at their default values. Which pair of features results in higher accuracy?\n",
    "\n",
    "**Soln. 11**: Training the kNN classifier with default parameters and setting k = 5, we obtain the following prediction accuracies per each pair of features:\n",
    "\n",
    "Sepal Length + Sepal Width (0, 1): 0.94\n",
    "\n",
    "Sepal Length + Petal Length (0, 2): 0.98\n",
    "\n",
    "Sepal Length + Petal Width (0, 3): 0.94\n",
    "\n",
    "Sepal Width + Petal Length (1, 2): 0.94\n",
    "\n",
    "Sepal Width + Petal Width (1, 3): 0.94\n",
    "\n",
    "Petal Length + Petal Width (2, 3): 0.94\n",
    "\n",
    "From the above results, we see that training our kNN Classifier using the first and third features (sepal length and petal length) yields the greatest prediction accuracy (98%) out of all possible pairs of features, with all other feature pairs yielding the same prediction accuracy (94%) for this instance, implying that neither of these pairs are better indicators of Iris type than the others.  \n",
    "\n",
    "**Question 12**. Train kNN classifier using all 4 features. Report the accuracy on test data set. Play with parameters of kNN to try to find a combination that results in the highest accuracy. Can you find something that works better than $k=3$ and default choices?\n",
    "\n",
    "**Soln 12**: The accuracies pertaining to each choice of `k` (applied to the same testing & training instances of the data with default metrics) are as follows:\n",
    "\n",
    "k=1: 0.94 (distance weights: 0.94)\n",
    "\n",
    "k=3: 0.96 (distance weights: 0.96)\n",
    "\n",
    "k=5: 0.98 (distance weights: 0.96)\n",
    "\n",
    "k=15: 0.96 (distance weights: 0.96)\n",
    "\n",
    "k=25: 0.94 (distance weights: 0.96)\n",
    "\n",
    "k=50: 0.9 (distance weights: 0.94)\n",
    "\n",
    "Based on the above statistics, it appears that when `k = 5` and `weights = 'uniform'`, our predictor achieves its greatest accuracy for this instance. Moreover, it appears that using 'distance' weights for above values of `k` exceeding 15 generally increases accuracy by at least a couple of percentage points. \n",
    "\n",
    "In general, upon applying this method across different testing & training instances and values for `k`, `weights`, and `metric` (namely the Euclidiean, Manhattan and Chebyshev metrics), we observe the following:\n",
    "\n",
    "1. Very high prediction accuracies across all values of `k` setting `weights='distance'` in tandem with the Manhattan metric (98% accuracy in each case).\n",
    "2. Even greater prediction accuracies setting `weights='distance'` in tandem with the Chebyshev metric (98% accuracy for k = 1, 3, 25, 50, and 100% accuracy for k = 5 and 15). \n",
    "3. Decreased accuracy when using the above parameters in tandem with setting `weights=uniform`. \n",
    "\n",
    "Based on the observations reported above, we can expect that on average, using `distance` weighting in our predictions with either the Manhattan or Chebysheve metrics will yield the greatest prediction accuracies from the `kNeighborsClassifier` on subsets of the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 11 Code\n",
    "#recalibrate data set to include different feature pairs\n",
    "# features = [sepal length, sepal width, petal length, petal width]\n",
    "i = [0, 2] # determines which features are being used\n",
    "X = iris.data[:, i]  # train on new features\n",
    "y = iris.target\n",
    "#resplit data for training & testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: \n",
      "[(2, 2), (1, 2), (1, 1), (0, 0), (0, 0), (1, 1), (2, 2), (2, 2), (1, 1), (2, 2), (2, 2), (1, 1), (2, 2), (1, 1), (2, 2), (1, 1), (2, 2), (1, 2), (0, 0), (0, 0), (1, 1), (0, 0), (1, 1), (2, 2), (1, 2), (1, 1), (1, 1), (0, 0), (2, 2), (1, 2), (0, 0), (0, 0), (1, 1), (2, 2), (1, 1), (2, 2), (1, 1), (0, 0), (0, 0), (0, 0), (1, 1), (0, 0), (1, 2), (0, 0), (1, 1), (1, 1), (1, 2), (0, 0), (0, 0), (0, 0)]\n",
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Question 11 Code (cont.)\n",
    "k = 5\n",
    "\n",
    "#recalibrate predictor\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k, weights='uniform')\n",
    "predictor.fit(X_train, y_train)\n",
    "\n",
    "#run new tests\n",
    "yhat = predictor.predict(X_test) # Find predictions for all test samples\n",
    "res = list(zip(yhat, y_test)) #join predictions with corresponding true values\n",
    "print(\"Test results: \")\n",
    "print(res)\n",
    "\n",
    "#print accuracy\n",
    "acc = predictor.score(X_test,y_test)\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 12 Code\n",
    "#recalibrate data set to include all features\n",
    "X = iris.data[:, [0,1, 2, 3]]  # train on all features\n",
    "y = iris.target\n",
    "#resplit data for training & testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor results: \n",
      "[(1, 1), (1, 2), (2, 2), (2, 2), (2, 2), (1, 1), (1, 2), (1, 2), (1, 1), (2, 2), (2, 2), (1, 1), (2, 2), (0, 0), (0, 0), (0, 0), (2, 2), (1, 1), (0, 0), (0, 0), (1, 1), (1, 1), (1, 1), (0, 0), (0, 0), (1, 1), (1, 1), (1, 1), (2, 2), (0, 0), (2, 2), (1, 1), (2, 2), (2, 2), (2, 2), (0, 0), (1, 1), (1, 1), (2, 1), (0, 0), (2, 2), (0, 0), (2, 2), (0, 0), (1, 1), (0, 0), (1, 1), (1, 1), (0, 0), (1, 1)]\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Question 12 Code (cont.)\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "#change num neighbors\n",
    "#k = 1\n",
    "#k = 3\n",
    "#k = 5\n",
    "#k = 15\n",
    "#k = 25\n",
    "k = 50\n",
    "\n",
    "\n",
    "\n",
    "#def weightedSepPetLen(x, y):\n",
    "# method calculating distance in a way that places greater importance on \n",
    "# distance along Sepal + Petal Length\n",
    "#    if (((x[0] - y[0])**2)+((x[2] - y[2])**2)) <= (((x[1]-y[1])**2)+((x[3]-y[3])**2)):\n",
    "#        d = ((x[0] - y[0])**2)/10+((x[1]-y[1])**2)+((x[2] - y[2])**2)/10+((x[3]-y[3])**2)\n",
    "#    else:\n",
    "#        d = (x[0] - y[0])**2+(x[1]-y[1])**2+(x[2] - y[2])**2+(x[3]-y[3])**2\n",
    "#    return math.sqrt(d)\n",
    "\n",
    "#recalibrate predictor\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k, weights='uniform', metric='chebyshev')\n",
    "predictor.fit(X_train, y_train)\n",
    "\n",
    "#print predictor results\n",
    "yhat = predictor.predict(X_test) # Find predictions for all test samples\n",
    "res = list(zip(yhat, y_test)) #join predictions with corresponding true values\n",
    "print(\"Predictor results: \")\n",
    "print(res)\n",
    "\n",
    "#print accuracy\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Write your own function for kNN (50% of the score)\n",
    "\n",
    "**Question 13** Write a function for k-nearest neighbor (k-NN) classification of the form `accuracy = knnC(X_test, y_test, X_train, y_train, k)`, where `k` is the number of nearest neighbors. Assume the Euclidean distance. So, the inputs are training and test data and the output should be accuracy on test data. Repeat **Question 9** using your function. Check if the results are the same.\n",
    "\n",
    "**NOTE:** I know that you can find python code for this on Web. However, I specifically ask you to write this function on your own and not use anybody's help. This is one of the rare ML algorithms that can be quickly implemented. I think you will learn a lot from this experience that will be very useful for the remainder of this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: \n",
      "[(1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (1, 1), (2, 2), (1, 2), (1, 1), (2, 2), (2, 2), (1, 1), (2, 2), (0, 0), (0, 0), (0, 0), (2, 2), (1, 1), (0, 0), (0, 0), (1, 1), (1, 1), (1, 1), (0, 0), (0, 0), (1, 1), (1, 1), (1, 1), (2, 2), (0, 0), (2, 2), (1, 1), (2, 2), (2, 2), (2, 2), (0, 0), (1, 1), (1, 1), (1, 1), (0, 0), (2, 2), (0, 0), (2, 2), (0, 0), (1, 1), (0, 0), (1, 1), (1, 1), (0, 0), (1, 1)]\n",
      "knnC reported accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Write new kNN function for Question 13 -CZ\n",
    "import math\n",
    "def sortDist(x): \n",
    "# method returning key value used for sorting in knnC\n",
    "    return x[0] # assumes x[0] is distance field associated with x\n",
    "def get_accuracy(results): \n",
    "# method returning calculated accuracy of knnC predictions contained in results\n",
    "    c = 0 # counter for correct predictions\n",
    "    for r in results:\n",
    "        if r[0] == r[1]: #compare predicted and true target values\n",
    "            c+=1\n",
    "    return c/len(results)\n",
    "def EucDist(x, y):\n",
    "# method returning Euclidean distance btwn points x & y\n",
    "    d = 0 # distance variable\n",
    "    for i in range(len(x)): # calculate Euclidean distance using all features\n",
    "        d += (x[i]-y[i])**2 # summate squared difference btwn feature values\n",
    "    return math.sqrt(d)\n",
    "\n",
    "def predict(n, k):\n",
    "# method returning predicted target \n",
    "# upon finding the majority target class in list of neighbors n\n",
    "    t0, t1, t2 = 0, 0, 0 #counters for each target label\n",
    "    for i in range(k): # count frequency of each target\n",
    "        if n[i][1] == 0: # target 0\n",
    "            t0+=1\n",
    "        elif n[i][1] == 1: #target 1\n",
    "            t1+=1\n",
    "        else: #target 2\n",
    "            t2+=1\n",
    "    # determine target with highest frequency\n",
    "    max = t0\n",
    "    p = 0 # prediction variable\n",
    "    if t1 > max:\n",
    "        max = t1\n",
    "        p = 1\n",
    "    if t2 > max:\n",
    "        p = 2\n",
    "    return p\n",
    "\n",
    "def knnC(X_test, y_test, X_train, y_train, k):\n",
    "# Given test data set X_test (with targets y_test), training data set X_train (w targets y_train),\n",
    "# and number of neighbors k, run k Nearest Neighbor algorithm (using Euclidean distance)\n",
    "# and return prediction accuracy. \n",
    "    pred = list() #list to collect predictions for x in X_test\n",
    "    for x in X_test: # iterate through testing set \n",
    "        n = list() #list to collect distance of neighbors and target values\n",
    "        for i in range(len(X_train)): # iterate through training set\n",
    "            xx = X_train[i] # next neighbor to examine\n",
    "            n.append((EucDist(x, xx), y_train[i])) # add distance and target for xx to n\n",
    "        # sort neighbors according to distance\n",
    "        n.sort(key=sortDist)\n",
    "        # begin to find majority target within first k entries of n\n",
    "        t0, t1, t2 = 0, 0, 0 #counters for each target label\n",
    "        for i in range(k): # count frequency of each target\n",
    "            if n[i][1] == 0: # target 0\n",
    "                t0+=1\n",
    "            elif n[i][1] == 1: #target 1\n",
    "                t1+=1\n",
    "            else: #target 2\n",
    "                t2+=1\n",
    "        # determine prediction based on target with highest frequency\n",
    "        p = predict(n, k)\n",
    "        pred.append(p) # add to list of predictions for this entry x\n",
    "    # Calculate accuracy     \n",
    "    result = list(zip(pred, y_test)) #join predictions with corresponding true values     \n",
    "    acc = get_accuracy(result)\n",
    "    print(\"Results: \")\n",
    "    print(result)\n",
    "    return acc\n",
    "\n",
    "k = 1 # number of nearest neighbors examined\n",
    "accuracy = knnC(X_test, y_test, X_train, y_train, k)\n",
    "print(\"knnC reported accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln. 9 (repeat)**: The accuracies reported by the knnC algorithm for each choice of `k` (applied to the exact same testing & training instances of the data) are as follows:\n",
    "\n",
    "k=1: 0.98\n",
    "\n",
    "k=3: 0.98\n",
    "\n",
    "k=5: 0.98\n",
    "\n",
    "k=15: 0.98\n",
    "\n",
    "k=25: 0.96\n",
    "\n",
    "k=50: 0.9\n",
    "\n",
    "Ultimately, running this algorithm in tandem with that for the `sklearn.neighbors.KNeighborsClassifier` appears to yield the exact same results. With respect to the data training & testing sets used for the above instances, our results suggest that the greatest prediction accuracy is obtained for the above values of k that do not exceed 15; beyond this point, we observe a drop in prediction accuracy. However, speaking from experience, we can expect that running our knnC algorithm on different instances of training & testing sets with `k=5` will most likely yield the greatest prediction accuracy on average, due to the similarities we've already detected between at least 2 target classes (green and blue) along at least 2 features (sepal length and width). For this same reason, these results align with expectation based on the same arguments made previously. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
